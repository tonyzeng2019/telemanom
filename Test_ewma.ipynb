{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tony_zeng/Academics_NTU/y4s1/fyp/telemanom/telemanom/_globals.py:29: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  dictionary = yaml.load(f.read())\n",
      "/Users/tony_zeng/Academics_NTU/y4s1/fyp/telemanom/telemanom/_globals.py:29: YAMLLoadWarning:\n",
      "\n",
      "calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "\n",
      "/Users/tony_zeng/Academics_NTU/y4s1/fyp/telemanom/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning:\n",
      "\n",
      "compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import operator\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "import scipy.stats as stats\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from telemanom._globals import Config\n",
    "import telemanom.errors as err\n",
    "import telemanom.helpers as helpers\n",
    "import telemanom.modeling as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('data/2018-05-19_15.00.10/y_hat/A-1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_original = np.load('data/test/A-1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8380,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8640, 25)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run(config, _id, logger):\n",
    "    ''' Top-level function for running experiment.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Parameters for modeling, execution levels, and error calculations loaded from config.yaml\n",
    "        _id (str): Unique id for each processing run generated from current time\n",
    "        logger (obj): Logger obj from logging module\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    '''\n",
    "\n",
    "    stats = {\n",
    "        \"true_positives\": 0,\n",
    "        \"false_positives\": 0,\n",
    "        \"false_negatives\": 0\n",
    "    }\n",
    "\n",
    "    with open(\"labeled_anomalies.csv\", \"rU\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        with open(\"results/%s.csv\" %_id, \"a\") as out:\n",
    "\n",
    "            writer = csv.DictWriter(out, config.header) # line by line results written to csv\n",
    "            writer.writeheader()\n",
    "        \n",
    "            for i, anom in enumerate(reader):\n",
    "                if reader.line_num >= 1:\n",
    "\n",
    "                    anom['run_id'] = _id\n",
    "                    logger.info(\"Stream # %s: %s\" %(reader.line_num-1, anom['chan_id']))\n",
    "                    model = None\n",
    "\n",
    "                    X_train, y_train, X_test, y_test = helpers.load_data(anom)\n",
    "                    \n",
    "                    # Generate or load predictions\n",
    "                    # ===============================\n",
    "                    y_hat = []\n",
    "                    if config.predict:\n",
    "                        model = models.get_model(anom, X_train, y_train, logger, train=config.train)\n",
    "                        y_hat = models.predict_in_batches(y_test, X_test, model, anom)\n",
    "                            \n",
    "                    else:\n",
    "                        y_hat = [float(x) for x in list(np.load(os.path.join(\"data\", config.use_id, \"y_hat\", anom[\"chan_id\"] + \".npy\")))]\n",
    "\n",
    "                    # Error calculations\n",
    "                    # ====================================================================================================\n",
    "                    e = err.get_errors(y_test, y_hat, anom, smoothed=False)\n",
    "                    e_s = err.get_errors(y_test, y_hat, anom, smoothed=True)\n",
    "\n",
    "                    anom[\"normalized_error\"] = np.mean(e) / np.ptp(y_test)\n",
    "                    logger.info(\"normalized prediction error: %s\" %anom[\"normalized_error\"])\n",
    "\n",
    "                    # Error processing (batch)\n",
    "                    # =========================\n",
    "\n",
    "                    E_seq, E_seq_scores = err.process_errors(y_test, y_hat, e_s, anom, logger)\n",
    "                    anom['scores'] = E_seq_scores\n",
    "\n",
    "                    anom = err.evaluate_sequences(E_seq, anom)\n",
    "                    anom[\"num_values\"] = y_test.shape[0] + config.l_s + config.n_predictions\n",
    "\n",
    "                    for key, value in stats.items():\n",
    "                        stats[key] += anom[key]\n",
    "\n",
    "                    helpers.anom_stats(stats, anom, logger)\n",
    "                    writer.writerow(anom)\n",
    "                break\n",
    "\n",
    "    helpers.final_stats(stats, logger)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config(\"config.yaml\")\n",
    "    _id = dt.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "    helpers.make_dirs(_id)  \n",
    "    logger = helpers.setup_logging(config, _id)\n",
    "    run(config, _id, logger)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
