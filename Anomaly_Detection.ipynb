{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1. Load Configurations from a yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tony_zeng/Academics_NTU/y4s1/fyp/telemanom/venv/lib/python3.5/site-packages/ipykernel_launcher.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "class Config:\n",
    "    '''Loads parameters from config.yaml into global object'''\n",
    "\n",
    "    def __init__(self, path_to_config):\n",
    "        \n",
    "        if os.path.isfile(path_to_config):    \n",
    "            pass\n",
    "        else:\n",
    "            print(\"No configuration path found.\")\n",
    "\n",
    "        setattr(self, \"path_to_config\", path_to_config)\n",
    "\n",
    "        dictionary = None\n",
    "        \n",
    "        with open(path_to_config, \"r\") as f:\n",
    "            dictionary = yaml.load(f.read())\n",
    "                \n",
    "        try:\n",
    "            for k,v in dictionary.items():\n",
    "                setattr(self, k, v)\n",
    "        except:\n",
    "            for k,v in dictionary.iteritems():\n",
    "                setattr(self, k, v)\n",
    "                \n",
    "# init config class\n",
    "config = Config(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2. Load & Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def load_preprocess_data(train_path, test_path):\n",
    "    '''Load & Preprocess data\n",
    "\n",
    "    Returns:\n",
    "        X_train (np array): array of train inputs with dimensions [timesteps, l_s, input dimensions]\n",
    "        y_train (np array): array of train outputs corresponding to true values following each sequence\n",
    "        X_test (np array): array of test inputs with dimensions [timesteps, l_s, input dimensions)\n",
    "        y_test (np array): array of test outputs corresponding to true values following each sequence\n",
    "    '''\n",
    "    train = np.load(train_path)\n",
    "    test = np.load(test_path)\n",
    "    # shape, split data\n",
    "    X_train, y_train = shape_data(train)\n",
    "    X_test, y_test = shape_data(test, train=False)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def shape_data(arr, train=True):\n",
    "    '''Shape raw input streams for ingestion into LSTM. config.l_s specifies the sequence length of \n",
    "    prior timesteps fed into the model at each timestep t. \n",
    "\n",
    "    Args:\n",
    "        arr (np array): array of input streams with dimensions [timesteps, 1, input dimensions]\n",
    "        train (bool): If shaping training data, this indicates data can be shuffled\n",
    "\n",
    "    Returns:\n",
    "        X (np array): array of inputs with dimensions [timesteps, l_s, input dimensions)\n",
    "        y (np array): array of outputs corresponding to true values following each sequence. \n",
    "            shape = [timesteps, n_predictions, 1)\n",
    "        l_s (int): sequence length to be passed to test shaping (if shaping train) so they are consistent\n",
    "    '''\n",
    "    \n",
    "    # print(\"LEN ARR: %s\" %len(arr))\n",
    "\n",
    "    data = [] \n",
    "    for i in range(len(arr) - config.l_s - config.n_predictions):\n",
    "        data.append(arr[i:i + config.l_s + config.n_predictions])\n",
    "    data = np.array(data) \n",
    "\n",
    "    assert len(data.shape) == 3\n",
    "\n",
    "    if train == True:\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "    X = data[:,:-config.n_predictions,:]\n",
    "    y = data[:,-config.n_predictions:,0] #telemetry value is at position 0\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(anom):\n",
    "    '''Load train and test data from repo. If not in repo need to download from source.\n",
    "\n",
    "    Args:\n",
    "        anom (dict): contains anomaly information for a given input stream\n",
    "\n",
    "    Returns:\n",
    "        X_train (np array): array of train inputs with dimensions [timesteps, l_s, input dimensions]\n",
    "        y_train (np array): array of train outputs corresponding to true values following each sequence\n",
    "        X_test (np array): array of test inputs with dimensions [timesteps, l_s, input dimensions)\n",
    "        y_test (np array): array of test outputs corresponding to true values following each sequence\n",
    "    '''\n",
    "    try:\n",
    "        train = np.load(os.path.join(\"data\", \"train\", anom['chan_id'] + \".npy\"))\n",
    "        test = np.load(os.path.join(\"data\", \"test\", anom['chan_id'] + \".npy\"))\n",
    "\n",
    "    except:\n",
    "        raise ValueError(\"Source data not found, may need to add data to repo: <link>\")\n",
    "\n",
    "    # shape, split data\n",
    "    X_train, y_train = shape_data(train)\n",
    "    X_test, y_test = shape_data(test, train=False)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def shape_data(arr, train=True):\n",
    "    '''Shape raw input streams for ingestion into LSTM. config.l_s specifies the sequence length of \n",
    "    prior timesteps fed into the model at each timestep t. \n",
    "\n",
    "    Args:\n",
    "        arr (np array): array of input streams with dimensions [timesteps, 1, input dimensions]\n",
    "        train (bool): If shaping training data, this indicates data can be shuffled\n",
    "\n",
    "    Returns:\n",
    "        X (np array): array of inputs with dimensions [timesteps, l_s, input dimensions)\n",
    "        y (np array): array of outputs corresponding to true values following each sequence. \n",
    "            shape = [timesteps, n_predictions, 1)\n",
    "        l_s (int): sequence length to be passed to test shaping (if shaping train) so they are consistent\n",
    "    '''\n",
    "    \n",
    "    # print(\"LEN ARR: %s\" %len(arr))\n",
    "\n",
    "    data = [] \n",
    "    for i in range(len(arr) - config.l_s - config.n_predictions):\n",
    "        data.append(arr[i:i + config.l_s + config.n_predictions])\n",
    "    data = np.array(data) \n",
    "\n",
    "    assert len(data.shape) == 3\n",
    "\n",
    "    if train == True:\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "    X = data[:,:-config.n_predictions,:]\n",
    "    y = data[:,-config.n_predictions:,0] #telemetry value is at position 0\n",
    "\n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
